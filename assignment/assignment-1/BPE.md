æˆ‘ä»¬æ¥å†™ä¸€ä¸ª **å®Œæ•´çš„ã€å¯è¿è¡Œçš„ BPE åˆ†è¯å™¨ç¨‹åº**ï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œéƒ½åŠ ä¸Šâ€œä¿å§†çº§â€æ³¨é‡Šã€‚

> âœ… è¯´æ˜ï¼šä¸ºäº†è®©ä½ èƒ½å¿«é€Ÿè¿è¡Œå’Œç†è§£ï¼Œæˆ‘ä»¬ä¸ç”¨ 1GB çš„ enwik9ï¼Œè€Œæ˜¯ç”¨ä¸€å°æ®µè‹±æ–‡å¥å­æ¨¡æ‹ŸçœŸå®è®­ç»ƒè¿‡ç¨‹ã€‚

---

## âœ… å®Œæ•´ä»£ç  + è¶…è¯¦ç»†æ³¨é‡Š

```python
# =============================
# ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥éœ€è¦ç”¨åˆ°çš„å·¥å…·åŒ…
# =============================

# æˆ‘ä»¬éœ€è¦ defaultdictï¼Œå®ƒå¯ä»¥è‡ªåŠ¨ç»™æ–°é”®è®¾ç½®é»˜è®¤å€¼ï¼ˆæ¯”å¦‚ 0 æˆ– []ï¼‰
# æ¯”å¦‚ï¼šcounts['æ–°å•è¯'] += 1ï¼Œå³ä½¿è¿™ä¸ªå•è¯æ²¡å‡ºç°è¿‡ä¹Ÿä¸ä¼šæŠ¥é”™
from collections import defaultdict

# =============================
# ç¬¬äºŒæ­¥ï¼šå®šä¹‰æˆ‘ä»¬è¦å­¦ä¹ çš„æ–‡æœ¬ï¼ˆæ¨¡æ‹ŸçœŸå®è¯­æ–™ï¼‰
# =============================

# è¿™æ˜¯æˆ‘ä»¬è¦è®­ç»ƒ BPE çš„â€œå°è¯¾æœ¬â€
# å°±åƒä½ å­¦è‹±è¯­æ—¶èƒŒçš„å‡ å¥è¯ï¼Œæ¨¡å‹ä¹Ÿä»è¿™é‡Œå­¦æ€ä¹ˆæ‹†è¯
text = """
hello world hello
how are you
i am fine thank you
hello python world
"""

# =============================
# ç¬¬ä¸‰æ­¥ï¼šé¢„å¤„ç†æ–‡æœ¬ï¼ˆæ¸…æ´— + åˆ‡è¯ + ç»Ÿè®¡é¢‘ç‡ï¼‰
# =============================

# æŠŠæ‰€æœ‰å­—æ¯å˜æˆå°å†™ï¼ˆé¿å… "Hello" å’Œ "hello" è¢«å½“æˆä¸¤ä¸ªè¯ï¼‰
text = text.lower()

# æŠŠå¤šä½™çš„ç©ºæ ¼ã€æ¢è¡Œç¬¦å˜æˆä¸€ä¸ªç©ºæ ¼ï¼Œæ–¹ä¾¿åˆ‡è¯
# strip() å»æ‰å¼€å¤´ç»“å°¾ç©ºæ ¼ï¼Œsplit() æŒ‰ç©ºæ ¼åˆ‡å¼€ï¼Œ' '.join() å†ç”¨ä¸€ä¸ªç©ºæ ¼è¿èµ·æ¥
text = ' '.join(text.strip().split())

# æŠŠæ–‡æœ¬æŒ‰ç©ºæ ¼åˆ‡åˆ†æˆä¸€ä¸ªä¸ªå•è¯ï¼Œæ¯”å¦‚ ['hello', 'world', 'hello', ...]
words = text.split()

# åˆ›å»ºä¸€ä¸ªâ€œè¯é¢‘å­—å…¸â€ï¼Œç”¨æ¥è®°å½•æ¯ä¸ªè¯å‡ºç°äº†å‡ æ¬¡
# defaultdict(int) è¡¨ç¤ºï¼šå¦‚æœè¿™ä¸ªå•è¯è¿˜æ²¡è®°è¿‡ï¼Œé»˜è®¤æ¬¡æ•°æ˜¯ 0
word_freqs = defaultdict(int)

# éå†æ¯ä¸€ä¸ªå•è¯ï¼Œç»™å®ƒçš„è®¡æ•° +1
for word in words:
    # æ¯”å¦‚ç¬¬ä¸€æ¬¡é‡åˆ° 'hello'ï¼Œword_freqs['hello'] å°±ä» 0 å˜æˆ 1
    # ç¬¬äºŒæ¬¡é‡åˆ°ï¼Œå°±å˜æˆ 2ï¼Œä¾æ­¤ç±»æ¨
    word_freqs[word] += 1

# ä½†æ˜¯ï¼BPE éœ€è¦çŸ¥é“ä¸€ä¸ªè¯ä»€ä¹ˆæ—¶å€™ç»“æŸ
# æ‰€ä»¥æˆ‘ä»¬ç»™æ¯ä¸ªè¯åé¢åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šæ ‡è®° </w>ï¼Œè¡¨ç¤ºâ€œè¿™ä¸ªè¯åˆ°æ­¤ä¸ºæ­¢â€
# æ¯”å¦‚ "hello" å˜æˆ "hello</w>"ï¼Œè¿™æ ·æ¨¡å‹å°±çŸ¥é“è¿™ä¸æ˜¯ "helloxxx" çš„ä¸€éƒ¨åˆ†

# æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„å­—å…¸ï¼Œä¿å­˜åŠ äº† </w> çš„è¯å’Œå®ƒä»¬çš„é¢‘ç‡
word_freqs_with_end = {}
for word, freq in word_freqs.items():
    # æŠŠåŸæ¥çš„è¯ + </w>ï¼Œæ¯”å¦‚ "hello" -> "hello</w>"
    word_with_end = word + "</w>"
    # å­˜è¿›æ–°å­—å…¸
    word_freqs_with_end[word_with_end] = freq

# ç°åœ¨æˆ‘ä»¬çš„â€œè¯¾æœ¬â€é•¿è¿™æ ·ï¼š
# {'hello</w>': 2, 'world</w>': 2, 'how</w>': 1, 'are</w>': 1, ...}

# =============================
# ç¬¬å››æ­¥ï¼šæŠŠæ¯ä¸ªè¯æ‹†æˆâ€œå­—ç¬¦â€ï¼ˆBPE çš„èµ·ç‚¹ï¼‰
# =============================

# åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œä¿å­˜æ¯ä¸ªè¯æ˜¯æ€ä¹ˆè¢«æ‹†æˆå­—ç¬¦çš„
splits = {}

# éå†æ¯ä¸ªâ€œåŠ äº† </w>â€çš„è¯
for word in word_freqs_with_end:
    # æŠŠè¿™ä¸ªè¯çš„æ¯ä¸€ä¸ªå­—æ¯ï¼ˆå’Œ</w>ï¼‰æ‹†å¼€æ”¾è¿›ä¸€ä¸ªåˆ—è¡¨
    # æ¯”å¦‚ "hello</w>" â†’ ['h', 'e', 'l', 'l', 'o', '<', '/', 'w', '>']
    split_into_chars = list(word)
    # æŠŠè¿™ä¸ªæ‹†åˆ†ç»“æœå­˜èµ·æ¥
    splits[word] = split_into_chars

# ç°åœ¨ splits é•¿è¿™æ ·ï¼š
# {
#   'hello</w>': ['h','e','l','l','o','<','/','w','>'],
#   'world</w>': ['w','o','r','l','d','<','/','w','>'],
#   ...
# }

# =============================
# ç¬¬äº”æ­¥ï¼šå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼šç»Ÿè®¡â€œå“ªä¸¤ä¸ªå­—ç¬¦ç»å¸¸æŒ¨åœ¨ä¸€èµ·â€
# =============================

def compute_pair_freqs(splits, word_freqs):
    """
    è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ï¼š
    æ‰«ææ‰€æœ‰è¯çš„å­—ç¬¦æ‹†åˆ†ç»“æœï¼Œç»Ÿè®¡â€œå“ªä¸¤ä¸ªå­—ç¬¦ç»å¸¸è¿åœ¨ä¸€èµ·å‡ºç°â€
    æ¯”å¦‚ï¼š'h' åé¢ç»å¸¸è·Ÿç€ 'e'ï¼Œ'e' åé¢ç»å¸¸è·Ÿç€ 'l'ï¼Œç­‰ç­‰
    """
    
    # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨æ¥è®°å½•æ¯ä¸€å¯¹å­—ç¬¦å‡ºç°äº†å¤šå°‘æ¬¡
    # æ¯”å¦‚ ('h','e'): 2, ('e','l'): 2, ...
    pair_freqs = defaultdict(int)
    
    # éå†æ¯ä¸€ä¸ªè¯ï¼ˆæ¯”å¦‚ 'hello</w>'ï¼‰
    for word, freq in word_freqs.items():
        # æ‹¿åˆ°è¿™ä¸ªè¯çš„å­—ç¬¦æ‹†åˆ†åˆ—è¡¨ï¼Œæ¯”å¦‚ ['h','e','l','l','o','<','/','w','>']
        split = splits[word]
        
        # éå†è¿™ä¸ªåˆ—è¡¨ï¼Œçœ‹æ¯ä¸¤ä¸ªâ€œæŒ¨ç€â€çš„å­—ç¬¦
        # i ä» 0 å¼€å§‹ï¼Œä¸€ç›´åˆ°å€’æ•°ç¬¬äºŒä¸ªå­—ç¬¦
        for i in range(len(split) - 1):
            # å–å‡ºç¬¬ i ä¸ªå­—ç¬¦å’Œç¬¬ i+1 ä¸ªå­—ç¬¦ï¼Œç»„æˆä¸€å¯¹
            # æ¯”å¦‚ i=0: ('h', 'e'), i=1: ('e', 'l'), i=2: ('l', 'l'), ...
            pair = (split[i], split[i+1])
            
            # è¿™å¯¹å­—ç¬¦å‡ºç°äº† freq æ¬¡ï¼ˆå› ä¸ºè¿™ä¸ªè¯å‡ºç°äº† freq æ¬¡ï¼‰
            # æ‰€ä»¥ç»™è¿™å¯¹å­—ç¬¦çš„æ€»é¢‘ç‡åŠ ä¸Š freq
            pair_freqs[pair] += freq
    
    # è¿”å›æ‰€æœ‰å­—ç¬¦å¯¹çš„é¢‘ç‡
    return pair_freqs

# =============================
# ç¬¬å…­æ­¥ï¼šå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼šæŠŠæœ€å¸¸è§çš„ä¸¤ä¸ªå­—ç¬¦â€œåˆå¹¶â€æˆä¸€ä¸ªæ–°ç¬¦å·
# =============================

def merge_pair(a, b, splits):
    """
    æŠŠæ‰€æœ‰è¿ç»­çš„ a å’Œ b åˆå¹¶æˆä¸€ä¸ªæ–°ä¸œè¥¿ï¼šab
    æ¯”å¦‚ a='l', b='l' â†’ åˆå¹¶æˆ 'll'
    """
    
    # éå†æ¯ä¸€ä¸ªè¯çš„å­—ç¬¦æ‹†åˆ†ç»“æœ
    for word in splits:
        # æ‹¿åˆ°å½“å‰è¯çš„å­—ç¬¦åˆ—è¡¨
        split = splits[word]
        # æ–°åˆ—è¡¨ï¼Œç”¨æ¥å­˜æ”¾åˆå¹¶åçš„ç»“æœ
        new_split = []
        i = 0  # å½“å‰æ‰«æåˆ°çš„ä½ç½®
        
        # ç”¨ while å¾ªç¯ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½ä¼šè·³è¿‡ä¸€äº›ä½ç½®
        while i < len(split):
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªå­—ç¬¦ï¼Œä¸”å½“å‰å­—ç¬¦æ˜¯ aï¼Œä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯ b
            if i < len(split) - 1 and split[i] == a and split[i+1] == b:
                # é‚£å°±æŠŠ a å’Œ b åˆå¹¶æˆ abï¼Œæ”¾è¿›æ–°åˆ—è¡¨
                new_split.append(a + b)
                # è·³è¿‡ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆå› ä¸ºå·²ç»åˆå¹¶äº†ï¼‰
                i += 2
            else:
                # å¦åˆ™ï¼ŒåªæŠŠå½“å‰å­—ç¬¦æ”¾è¿›å»
                new_split.append(split[i])
                i += 1
        
        # æ›´æ–°è¿™ä¸ªè¯çš„æ‹†åˆ†ç»“æœ
        splits[word] = new_split
    
    # è¿”å›æ›´æ–°åçš„ splits
    return splits

# =============================
# ç¬¬ä¸ƒæ­¥ï¼šå¼€å§‹è®­ç»ƒï¼é‡å¤â€œæ‰¾æœ€å¸¸è§å­—ç¬¦å¯¹ â†’ åˆå¹¶â€è¿™ä¸ªè¿‡ç¨‹
# =============================

# æˆ‘ä»¬æƒ³å­¦ä¹  10 æ¡åˆå¹¶è§„åˆ™ï¼ˆæ¯”å¦‚ 'l'+'l'â†’'ll', 'll'+'o'â†’'llo'...ï¼‰
num_merges = 10

# ç”¨æ¥ä¿å­˜æˆ‘ä»¬å­¦åˆ°çš„æ‰€æœ‰åˆå¹¶è§„åˆ™
# æ¯”å¦‚ {('l','l'): 'll', ('ll','o'): 'llo', ...}
merges = {}

# å¼€å§‹å¾ªç¯ï¼Œä¸€å…±åˆå¹¶ 10 æ¬¡
for step in range(num_merges):
    # ç¬¬1æ­¥ï¼šç»Ÿè®¡å½“å‰æ‰€æœ‰å­—ç¬¦å¯¹çš„é¢‘ç‡
    pair_freqs = compute_pair_freqs(splits, word_freqs_with_end)
    
    # å¦‚æœæ²¡æœ‰å­—ç¬¦å¯¹äº†ï¼Œå°±åœæ­¢
    if len(pair_freqs) == 0:
        print("æ²¡æœ‰æ›´å¤šå¯ä»¥åˆå¹¶çš„å­—ç¬¦å¯¹äº†ï¼")
        break
    
    # ç¬¬2æ­¥ï¼šæ‰¾å‡ºé¢‘ç‡æœ€é«˜çš„é‚£ä¸€å¯¹å­—ç¬¦
    # max() å‡½æ•°ï¼šä» pair_freqs ä¸­æ‰¾â€œå€¼æœ€å¤§â€çš„é‚£ä¸ªé”®
    # key=pair_freqs.get è¡¨ç¤ºï¼šæŒ‰ valueï¼ˆé¢‘ç‡ï¼‰æ¥æ¯”è¾ƒ
    best_pair = max(pair_freqs, key=pair_freqs.get)
    best_freq = pair_freqs[best_pair]
    
    # æ‰“å°æˆ‘ä»¬å­¦åˆ°äº†ä»€ä¹ˆ
    print(f"ç¬¬ {step+1} æ¬¡åˆå¹¶ï¼šæŠŠ '{best_pair[0]}' å’Œ '{best_pair[1]}' åˆå¹¶æˆ '{best_pair[0]+best_pair[1]}'ï¼Œå…±å‡ºç° {best_freq} æ¬¡")
    
    # è®°ä½è¿™æ¡è§„åˆ™
    merges[best_pair] = best_pair[0] + best_pair[1]
    
    # ç¬¬3æ­¥ï¼šçœŸæ­£æ‰§è¡Œåˆå¹¶
    splits = merge_pair(best_pair[0], best_pair[1], splits)

# =============================
# ç¬¬å…«æ­¥ï¼šæ„å»ºæœ€ç»ˆè¯æ±‡è¡¨ï¼ˆVocabularyï¼‰
# =============================

# è®­ç»ƒå®Œæˆåï¼Œæ”¶é›†æ‰€æœ‰å‡ºç°è¿‡çš„ tokenï¼ˆå­è¯å•å…ƒï¼‰
vocab = set()

# éå†æ‰€æœ‰è¯çš„æœ€ç»ˆæ‹†åˆ†ç»“æœï¼ŒæŠŠæ¯ä¸ª token åŠ å…¥è¯æ±‡è¡¨
for split in splits.values():
    for token in split:
        vocab.add(token)

# ä¹ŸåŠ å…¥ä¸€äº›åŸºç¡€å­—ç¬¦ï¼Œç¡®ä¿èƒ½è¦†ç›–æœªç™»å½•è¯
# è¿™é‡Œæˆ‘ä»¬å¯ä»¥æŠŠåŸå§‹å­—ç¬¦ä¹ŸåŠ è¿›å»ï¼Œæ¯”å¦‚ 'a', 'b', 'l' ç­‰
# æˆ–è€…æ›´ä¸¥è°¨åœ°ï¼šä»æ‰€æœ‰å•è¯ä¸­æå–å”¯ä¸€å­—ç¬¦
all_chars = set()
for word in word_freqs_with_end:
    for c in word:
        all_chars.add(c)
vocab.update(all_chars)

# æŠŠè¯æ±‡è¡¨è½¬æˆæ’åºåˆ—è¡¨ï¼Œä¾¿äºåˆ†é… ID
vocab = sorted(vocab)

print("\n" + "="*50)
print("æœ€ç»ˆè¯æ±‡è¡¨ï¼ˆå…± {} ä¸ª tokenï¼‰:".format(len(vocab)))
print("="*50)
print(vocab)

# =============================
# ç¬¬ä¹æ­¥ï¼šæ„å»º token <-> ID æ˜ å°„è¡¨
# =============================

# åˆ›å»ºä¸¤ä¸ªå­—å…¸ï¼š
# token_to_id: æŠŠ token æ˜ å°„æˆæ•°å­— IDï¼ˆæ¨¡å‹åªèƒ½å¤„ç†æ•°å­—ï¼‰
# id_to_token: æŠŠ ID è¿˜åŸæˆ tokenï¼ˆç”¨äºè§£ç ï¼‰

token_to_id = {token: idx for idx, token in enumerate(vocab)}
id_to_token = {idx: token for token, idx in token_to_id.items()}

print(f"\nToken åˆ° ID æ˜ å°„ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
for token in list(token_to_id.keys())[:10]:
    print(f"  '{token}' â†’ {token_to_id[token]}")

# =============================
# ç¬¬åæ­¥ï¼šå®šä¹‰ç¼–ç å‡½æ•°ï¼ˆEncode: æ–‡æœ¬ â†’ Token IDsï¼‰
# =============================

def encode(text):
    """
    å°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸º token ID åºåˆ—ã€‚
    ä½¿ç”¨â€œè´ªå¿ƒæœ€é•¿åŒ¹é…â€ç­–ç•¥ï¼šä¼˜å…ˆåŒ¹é…æœ€é•¿çš„ tokenã€‚
    """
    # æ·»åŠ è¯å°¾æ ‡è®°ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if not text.endswith("</w>"):
        text += "</w>"
    
    tokens = []
    i = 0
    n = len(text)
    
    while i < n:
        matched = False
        # ä»æœ€é•¿å¯èƒ½åŒ¹é…å¼€å§‹ï¼ˆè´ªå¿ƒï¼‰
        for j in range(n, i, -1):
            candidate = text[i:j]
            if candidate in token_to_id:
                tokens.append(candidate)
                i = j
                matched = True
                break
        if not matched:
            # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œä½¿ç”¨å•ä¸ªå­—ç¬¦ï¼ˆæˆ– <UNK>ï¼‰
            tokens.append(text[i])
            i += 1
    
    # è½¬ä¸º ID
    return [token_to_id[token] for token in tokens]

# =============================
# ç¬¬åä¸€æ­¥ï¼šå®šä¹‰è§£ç å‡½æ•°ï¼ˆDecode: Token IDs â†’ æ–‡æœ¬ï¼‰
# =============================

def decode(token_ids):
    """
    å°† token ID åºåˆ—è§£ç ä¸ºåŸå§‹å­—ç¬¦ä¸²ã€‚
    """
    # å…ˆè½¬æˆ token
    tokens = [id_to_token[tid] for tid in token_ids]
    # æ‹¼æ¥
    raw = ''.join(tokens)
    # å»æ‰è¯å°¾æ ‡è®° </w>
    if raw.endswith("</w>"):
        raw = raw[:-4]
    return raw

# =============================
# ç¬¬åäºŒæ­¥ï¼šæµ‹è¯•ç¼–ç ä¸è§£ç ï¼ˆå®Œæ•´é—­ç¯ï¼‰
# =============================

print("\n" + "="*50)
print("ç¼–ç ä¸è§£ç æµ‹è¯•ï¼ˆç«¯åˆ°ç«¯é—­ç¯ï¼‰")
print("="*50)

test_words = ["hello", "world", "how", "are", "python", "fine"]

for word in test_words:
    # ç¼–ç 
    encoded_ids = encode(word)
    # è§£ç 
    decoded_text = decode(encoded_ids)
    
    print(f"åŸå§‹: '{word}'")
    print(f"ç¼–ç : {encoded_ids}")
    print(f"tokens: {[id_to_token[tid] for tid in encoded_ids]}")
    print(f"è§£ç : '{decoded_text}'")
    print(f"ä¸€è‡´: {word == decoded_text}")
    print("-" * 40)
```


#GPT5
```
from collections import defaultdict

# å…¨å±€ merge è§„åˆ™è¡¨
merge_rules = []


def compare_pair_freqs(splits, word_freqs):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs


def merge_pairs(a, b, splits):
    for word in splits:
        split = splits[word]
        new_split = []
        i = 0
        while i < len(split):
            if i < len(split) - 1 and a == split[i] and b == split[i + 1]:
                new_split.append(a + b)
                i += 2
            else:
                new_split.append(split[i])
                i += 1
        splits[word] = new_split
    return splits


def bpe_train(splits, words_freq, vocab_size):
    """
    è®­ç»ƒ BPEï¼Œç›´åˆ°è¯è¡¨å¤§å°è¾¾åˆ° vocab_size
    """
    global merge_rules
    merge_rules = []

    while True:
        # å½“å‰è¯è¡¨å¤§å°
        current_vocab = set()
        for split in splits.values():
            for token in split:
                current_vocab.add(token)

        if len(current_vocab) >= vocab_size:
            print(f"è¾¾åˆ°ç›®æ ‡è¯è¡¨å¤§å° {vocab_size}ï¼Œåœæ­¢è®­ç»ƒã€‚")
            break

        pair_freqs = compare_pair_freqs(splits=splits, word_freqs=words_freq)
        if len(pair_freqs) == 0:
            print("æ²¡æœ‰å¯ä»¥åˆå¹¶çš„å­—ç¬¦å¯¹")
            break

        best_pair = max(pair_freqs, key=pair_freqs.get)
        best_freq = pair_freqs[best_pair]
        print(
            f"åˆå¹¶: '{best_pair[0]}' + '{best_pair[1]}' -> '{best_pair[0]+best_pair[1]}' (å‡ºç° {best_freq} æ¬¡)"
        )
        splits = merge_pairs(best_pair[0], best_pair[1], splits=splits)
        merge_rules.append(best_pair)

    return splits


def construct_vocabulary(splits, words_freq_with_end):
    vocabulary = set()
    for split in splits.values():
        vocabulary.update(split)
    for word in words_freq_with_end:
        vocabulary.update(list(word))
    vocabulary = sorted(vocabulary)
    vocabulary.append("<UNK>")
    return vocabulary


def construct_tokens_ids(vocabulary):
    token_to_id = {token: id for id, token in enumerate(vocabulary)}
    id_to_token = {id: token for token, id in token_to_id.items()}
    return token_to_id, id_to_token


def encode_bpe(word, token_to_id):
    """
    æŒ‰ merge_rules é¡ºåºæ‰§è¡Œåˆå¹¶ï¼Œè€Œä¸æ˜¯ longest match
    """
    if not word.endswith("</w>"):
        word += "</w>"
    split = list(word)

    for (a, b) in merge_rules:
        i = 0
        new_split = []
        while i < len(split):
            if i < len(split) - 1 and split[i] == a and split[i + 1] == b:
                new_split.append(a + b)
                i += 2
            else:
                new_split.append(split[i])
                i += 1
        split = new_split

    return [token_to_id.get(tok, token_to_id["<UNK>"]) for tok in split]


def decode(ids, id_to_token):
    tokens = [id_to_token[i] for i in ids]
    raw = "".join(tokens)
    if raw.endswith("</w>"):
        raw = raw[:-4]
    return raw


def main():
    text = """
    hello world hello
    how are you
    i am fine thank you
    hello python world
    """
    text = text.lower()
    words = text.strip().split()

    word_freqs = defaultdict(int)
    for word in words:
        word_freqs[word] += 1

    word_freqs_with_end = {w + "</w>": f for w, f in word_freqs.items()}

    splits = {word: list(word) for word in word_freqs_with_end}

    # ğŸš€ è®¾ç½®ç›®æ ‡è¯è¡¨å¤§å°
    splits = bpe_train(splits=splits, words_freq=word_freqs_with_end, vocab_size=50)

    vocabulary = construct_vocabulary(splits, word_freqs_with_end)
    token_to_id, id_to_token = construct_tokens_ids(vocabulary)

    print("æœ€ç»ˆè¯è¡¨å¤§å°:", len(vocabulary))
    print("éƒ¨åˆ†è¯è¡¨:", vocabulary[:30])

    test_words = ["hello", "world", "how", "are", "python", "fine"]
    for word in test_words:
        encoded_ids = encode_bpe(word, token_to_id)
        decoded_text = decode(encoded_ids, id_to_token)

        print(f"åŸå§‹: '{word}'")
        print(f"ç¼–ç : {encoded_ids}")
        print(f"tokens: {[id_to_token[tid] for tid in encoded_ids]}")
        print(f"è§£ç : '{decoded_text}'")
        print(f"ä¸€è‡´: {word == decoded_text}")
        print("-" * 40)


if __name__ == "__main__":
    main()

```