import torch
import torch.nn as nn


class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¸¦ Dropoutï¼‰
    å°±åƒä¸€åœºâ€œåªå‡†çœ‹è¿‡å»â€çš„èˆä¼šï¼Œæ¯ä¸ª token åªèƒ½å…³æ³¨å®ƒä¹‹å‰çš„ä¼™ä¼´ã€‚
    """

    def __init__(self, d_in, d_out, context_length, dropout, head_nums, qkv_bias=False):
        super().__init__()
        self.d_in = d_in          # è¾“å…¥ç»´åº¦ï¼šæ¯ä¸ª token çš„å‘é‡é•¿åº¦
        self.d_out = d_out        # è¾“å‡ºç»´åº¦ï¼šæ¯ä¸ªå¤´è¾“å‡ºçš„æ€»ç»´åº¦
        self.head_nums = head_nums  # å¤šå°‘ä¸ªâ€œæ³¨æ„åŠ›å°åˆ†é˜Ÿâ€ï¼ˆheadï¼‰
        self.context_length = context_length  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆç”¨äºæ©ç ï¼‰
        self.head_dim = d_out // head_nums   # æ¯ä¸ªå°åˆ†é˜Ÿçš„â€œæ™ºåŠ›å®¹é‡â€

        # æ¯ä¸ªå°åˆ†é˜Ÿéƒ½æœ‰è‡ªå·±çš„â€œè§‚å¯Ÿæ–¹å¼â€ï¼ˆçº¿æ€§æŠ•å½±ï¼‰
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)  # Queryï¼šæˆ‘æƒ³çœ‹è°ï¼Ÿ
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)  # Keyï¼šæˆ‘æœ‰ä»€ä¹ˆå¯è¢«çœ‹åˆ°çš„ï¼Ÿ
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)  # Valueï¼šæˆ‘çš„çœŸå®ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ

        # é¢„å…ˆåˆ¶ä½œä¸€å¼ â€œç¦æ­¢å·çœ‹æœªæ¥â€çš„é»‘åå•ï¼ˆå› æœæ©ç ï¼‰
        # ä¸Šä¸‰è§’ä¸º 1ï¼Œè¡¨ç¤ºâ€œä¸èƒ½çœ‹â€
        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
        self.register_buffer("mask", mask)  # æ°¸ä¹…ä¿å­˜ï¼Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°

        # æœ€åçš„â€œæ€»ç»“æŠ¥å‘Šâ€æŠ•å½±å±‚ï¼šæŠŠæ‰€æœ‰å°åˆ†é˜Ÿçš„ä¿¡æ¯æ±‡æ€»æˆæœ€ç»ˆè¾“å‡º
        self.proj = nn.Linear(d_out, d_out)

        # æ³¨æ„åŠ›æƒé‡éšæœºâ€œå¤±å¿†â€æœºåˆ¶ï¼ˆDropoutï¼‰ï¼Œé˜²æ­¢è¿‡åº¦å…³æ³¨æŸä¸ª token
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šè®©æ¯ä¸ª token å­¦ä¼šâ€œä»è¿‡å»ä¸­æå–é‡ç‚¹â€
        x: (batch_size, num_tokens, d_in)
        """
        batch_size, token_nums, d_in = x.shape

        # Step 1: æ¯ä¸ª token éƒ½ç”Ÿæˆè‡ªå·±çš„â€œæé—®â€ï¼ˆQueryï¼‰ã€â€œåç‰‡â€ï¼ˆKeyï¼‰å’Œâ€œçœŸå®å†…å®¹â€ï¼ˆValueï¼‰
        queries = self.W_query(x)  # æˆ‘æƒ³å…³æ³¨ä»€ä¹ˆï¼Ÿ
        keys    = self.W_key(x)    # æˆ‘æœ‰ä»€ä¹ˆå¯ä»¥è¢«å…³æ³¨çš„ï¼Ÿ
        values  = self.W_value(x)  # æˆ‘çš„çœŸå®ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ

        # Step 2: æŠŠå¤§å›¢é˜Ÿæ‹†åˆ†æˆå¤šä¸ªâ€œæ³¨æ„åŠ›å°åˆ†é˜Ÿâ€ï¼ˆå¤šå¤´æœºåˆ¶ï¼‰
        # ä» (b, n, d_out) â†’ (b, n, num_heads, head_dim)
        queries = queries.view(batch_size, token_nums, self.head_nums, self.head_dim)
        keys    = keys.view(batch_size, token_nums, self.head_nums, self.head_dim)
        values  = values.view(batch_size, token_nums, self.head_nums, self.head_dim)

        # Step 3: è°ƒæ•´é¡ºåºï¼Œè®©â€œå°åˆ†é˜Ÿâ€æˆä¸ºä¸»ç»´åº¦ï¼Œæ–¹ä¾¿å¹¶è¡Œè®¡ç®—
        # (b, n, h, d) â†’ (b, h, n, d)
        queries = queries.transpose(1, 2)
        keys    = keys.transpose(1, 2)
        values  = values.transpose(1, 2)

        # Step 4: è®¡ç®—â€œè°è¯¥å…³æ³¨è°â€çš„çƒ­åº¦å›¾ï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰
        # æ¯ä¸ª query å’Œæ‰€æœ‰ key æ‰“åˆ†ï¼šQ @ K^T
        attention_scores = queries @ keys.transpose(2, 3)  # (b, h, n, n)

        # Step 5: æ‹¿å‡ºâ€œé»‘åå•â€ï¼Œç¦æ­¢ä»»ä½• token å·çœ‹æœªæ¥ï¼
        # åªä¿ç•™å·¦ä¸‹è§’çš„åˆæ³•åŒºåŸŸï¼ˆå› æœæ©ç ï¼‰
        mask_bool = self.mask.bool()[:token_nums, :token_nums]  # åŠ¨æ€æˆªå–å½“å‰é•¿åº¦
        attention_scores.masked_fill_(mask_bool, -torch.inf)    # å·çœ‹è€…åˆ†æ•°å½’ -âˆ

        # Step 6: æŠŠçƒ­åº¦åˆ†è½¬æˆâ€œæ³¨æ„åŠ›æ¦‚ç‡â€ï¼ˆsoftmaxï¼‰ï¼Œå¹¶ç¼©æ”¾é˜²æ­¢çˆ†ç‚¸
        # é™¤ä»¥ sqrt(d_k) æ˜¯ä¸ºäº†è®© softmax æ›´ç¨³å®š
        attention_weight = torch.softmax(
            attention_scores / (self.head_dim ** 0.5), dim=-1
        )

        # Step 7: éšæœºè®©ä¸€äº›æ³¨æ„åŠ›è¿æ¥â€œå¤±å¿†â€ï¼ˆDropoutï¼‰ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        attention_weight = self.dropout(attention_weight)

        # Step 8: æŒ‰æ³¨æ„åŠ›æƒé‡ï¼Œå¯¹æ‰€æœ‰ value è¿›è¡Œâ€œåŠ æƒæ±‚å’Œâ€ â†’ å¾—åˆ°æ¯ä¸ª token çš„æ–°è¡¨ç¤º
        # å°±åƒæ ¹æ®é‡è¦æ€§é‡æ–°ç»„åˆä¿¡æ¯
        context_vec = (attention_weight @ values)  # (b, h, n, d)
        context_vec = context_vec.transpose(1, 2)  # æ¢å› (b, n, h, d)

        # Step 9: æŠŠæ‰€æœ‰å°åˆ†é˜Ÿçš„ä¿¡æ¯â€œæ‹¼æ¥â€èµ·æ¥
        # (b, n, h, d) â†’ (b, n, d_out)
        context_vec = context_vec.contiguous().view(batch_size, token_nums, self.d_out)

        # Step 10: æœ€åçš„â€œæ€»ç»“æŠ¥å‘Šâ€ï¼šç”¨ proj å±‚æ•´åˆä¿¡æ¯ï¼Œè¾“å‡ºæœ€ç»ˆç»“æœ
        return self.proj(context_vec)


# ğŸ‰ ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    mha = MultiHeadAttention(
        d_in=768,           # è¾“å…¥ç»´åº¦ï¼ˆå¦‚ BERT çš„ hidden sizeï¼‰
        d_out=768,          # è¾“å‡ºç»´åº¦
        context_length=1024, # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        dropout=0.1,         # æ³¨æ„åŠ› dropout æ¦‚ç‡
        head_nums=12,        # 12 ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆå¦‚ BERT-baseï¼‰
        qkv_bias=False
    )

    # æ¨¡æ‹Ÿä¸€æ‰¹ 2 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæœ‰ 5 ä¸ª token
    x = torch.randn(2, 5, 768)
    output = mha(x)
    print(output.shape)  # åº”è¯¥æ˜¯ torch.Size([2, 5, 768])